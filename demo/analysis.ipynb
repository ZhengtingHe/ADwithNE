{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from model import ParticleEventTransformer\n",
    "from data import get_database_path, get_h5_files, read_h5_file, select_events\n",
    "from utils import load_toml_config\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if sys.platform == \"darwin\" else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = get_database_path()\n",
    "bkg_files, sig_files = get_h5_files()\n",
    "\n",
    "\n",
    "# SM processes\n",
    "particle_type_scale = 20\n",
    "bkg = read_h5_file(database_path, bkg_files[0]['file'])\n",
    "# SM processes\n",
    "\n",
    "neutral_boson = read_h5_file(database_path, sig_files[0]['file'])\n",
    "# A neutral scalar boson (A) with mass 50 GeV, decaying to two off-shell Z bosons, each forced to decay to two leptons: A → 4l\n",
    "\n",
    "leptoquark = read_h5_file(database_path, sig_files[1]['file'])\n",
    "# A leptoquark (LQ) with mass 80 GeV, decaying to a b quark and a τ lepton24\n",
    "\n",
    "neutral_higgs = read_h5_file(database_path, sig_files[2]['file'])\n",
    "# A scalar boson with mass 60 GeV, decaying to two tau leptons: h0→ ττ\n",
    "\n",
    "charged_higgs = read_h5_file(database_path, sig_files[3]['file'])\n",
    "# A charged scalar boson with mass 60 GeV, decaying to a tau lepton and a neutrino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_size': 3, 'embed_size': 16, 'num_heads': 8, 'num_layers': 4, 'hidden_dim': 256, 'output_dim': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/desmondhe/anaconda3/envs/ad/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ParticleEventTransformer(\n",
       "  (particle_embedding): Linear(in_features=3, out_features=16, bias=True)\n",
       "  (pos_encoder): ParticlePositionalEncoding()\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=16, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=304, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hyper_parameters = load_toml_config(\"Transformer\")\n",
    "print(model_hyper_parameters)\n",
    "feature_size = model_hyper_parameters[\"feature_size\"]\n",
    "embed_size = model_hyper_parameters[\"embed_size\"]\n",
    "num_heads = model_hyper_parameters[\"num_heads\"]\n",
    "num_layers = model_hyper_parameters[\"num_layers\"]\n",
    "hidden_dim = model_hyper_parameters[\"hidden_dim\"]\n",
    "output_dim = model_hyper_parameters[\"output_dim\"]\n",
    "\n",
    "model = ParticleEventTransformer(feature_size, embed_size, num_heads, hidden_dim, output_dim, num_layers)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import EventDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_test_num = 100000\n",
    "bkg_infer_test = bkg[:infer_test_num]\n",
    "infer_dataset = EventDataset(bkg_infer_test)\n",
    "infer_dataloader = DataLoader(infer_dataset, batch_size=256, num_workers=16, prefetch_factor=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:01<00:00, 286.83it/s]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
    "\n",
    "embed_points = inference(model, infer_dataloader, embed_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
