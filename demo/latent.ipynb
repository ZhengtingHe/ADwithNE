{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from model import ParticleEventTransformer\n",
    "from data import get_database_path, get_h5_files, read_h5_file, select_events\n",
    "from utils import load_toml_config\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if sys.platform == \"darwin\" else \"cpu\"\n",
    "random_seed = 114514\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[369.1842041  370.31808472 327.48770142 326.72790527]\n",
      "[-377.18408203 -356.34307861 -320.93734741 -340.06167603]\n"
     ]
    }
   ],
   "source": [
    "EMD_config = load_toml_config(\"EMD\")\n",
    "particle_type_scale = EMD_config['particle_type_scale']\n",
    "\n",
    "model_hyper_parameters = load_toml_config(\"Transformer\")\n",
    "output_dim = model_hyper_parameters[\"output_dim\"]\n",
    "\n",
    "import h5py\n",
    "embedding_points = h5py.File(\"../embedding_points.h5\", \"r\")\n",
    "\n",
    "from analysis import Normalizer\n",
    "normalizer = Normalizer(*[value for value in embedding_points.values()])\n",
    "\n",
    "print(normalizer.max)\n",
    "print(normalizer.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['SM', 'charged_Higgs', 'leptoquark', 'neutral_Higgs', 'neutral_boson']>\n",
      "SM (13451915, 4)\n",
      "charged_Higgs (760272, 4)\n",
      "leptoquark (340544, 4)\n",
      "neutral_Higgs (691283, 4)\n",
      "neutral_boson (55969, 4)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_points.keys())\n",
    "for key in embedding_points.keys():\n",
    "    print(key, embedding_points[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(idx, num_classes):\n",
    "  return np.squeeze(np.eye(num_classes)[idx.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, embedding_points, normalizer, label):\n",
    "        points = np.concatenate([embedding_points[key] for key in embedding_points.keys()])\n",
    "        self.points = points\n",
    "        self.normalizer = normalizer\n",
    "        label_idx = np.concatenate([len(embedding_points[key]) * [i] for i, key in enumerate(embedding_points.keys())])\n",
    "        self.label = one_hot(label_idx, 5)\n",
    "        assert len(self.points) == len(self.label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.points)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.normalizer(self.points[idx]), self.label[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes):\n",
    "        super(MLP, self).__init__()\n",
    "        def make_layer(in_size, out_size):\n",
    "            layer = nn.Sequential(\n",
    "                nn.Linear(in_size, out_size),\n",
    "                nn.LeakyReLU(),\n",
    "                # nn.Dropout(0.1)\n",
    "            )\n",
    "            nn.init.kaiming_normal_(layer[0].weight, nonlinearity='leaky_relu')\n",
    "            return layer\n",
    "        self.layers = nn.Sequential(\n",
    "            make_layer(input_size, hidden_sizes[0]),\n",
    "            *[make_layer(hidden_sizes[i], hidden_sizes[i+1]) for i in range(len(hidden_sizes)-1)],\n",
    "            nn.Linear(hidden_sizes[-1], 5),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
