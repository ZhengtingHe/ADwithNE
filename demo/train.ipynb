{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime \n",
    "import numpy as np\n",
    "sys.path.append(\"..\")\n",
    "# from tqdm.rich import tqdm, trange\n",
    "from tqdm import tqdm, trange\n",
    "from copy import deepcopy\n",
    "import h5py\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchmetrics.regression import MeanAbsolutePercentageError\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from data_loader import get_database_path, get_h5_files, read_h5_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "random_seed = 114514\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Set default tensor type\n",
    "if sys.platform == \"darwin\":\n",
    "    # Mac OS\n",
    "    default_precision = torch.float32\n",
    "else:\n",
    "    # Linux or Windows\n",
    "    default_precision = torch.float64\n",
    "torch.set_default_dtype(default_precision)\n",
    "\n",
    "# Set device\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if sys.platform == \"darwin\" else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = get_database_path()\n",
    "bkg_files, sig_files = get_h5_files()\n",
    "\n",
    "\n",
    "# SM processes\n",
    "bkg = read_h5_file(database_path, bkg_files[0]['file'])\n",
    "bkg_pairs, bkg_emds = read_h5_file(database_path, \"bkg_emds.h5\", datatype='EMD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedEventsDataset(Dataset):\n",
    "    def __init__(self, events, pairs, emds):\n",
    "        assert len(emds) == len(pairs)\n",
    "        assert len(pairs.shape) == 2 and pairs.shape[1] == 2\n",
    "        self.events = torch.from_numpy(events)\n",
    "        self.pairs = torch.from_numpy(pairs)\n",
    "        self.emds = torch.from_numpy(emds)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.events[self.pairs[idx][0]], self.events[self.pairs[idx][1]], self.emds[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_dataset = PairedEventsDataset(bkg, bkg_pairs, bkg_emds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        def make_layer(in_size, out_size):\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(in_size, out_size),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(0.1)\n",
    "            )\n",
    "        self.layers = nn.Sequential(\n",
    "            make_layer(input_size, hidden_sizes[0]),\n",
    "            *[make_layer(hidden_sizes[i], hidden_sizes[i+1]) for i in range(len(hidden_sizes)-1)],\n",
    "            nn.Linear(hidden_sizes[-1], output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss & Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Euclidean_distance(y1, y2):\n",
    "    return torch.norm(y1 - y2, dim=1)\n",
    "\n",
    "def hyperbolic_distance(y1, y2):\n",
    "    return torch.acosh(1 + 2 * torch.sum((y1 - y2)**2) / ((1 - torch.sum(y1**2)) * (1 - torch.sum(y2**2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 0.], dtype=torch.float32)\n",
      "tensor([1., 1., 0.], dtype=torch.float32)\n",
      "tensor([0.0588, 0.0588, 0.0000], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[ 1, 3], [-1, 4], [2, 2]] , dtype=torch.float)\n",
    "b = torch.tensor([[ 1, 4], [-1, 3], [2, 2]] , dtype=torch.float)\n",
    "print(torch.norm(a-b, dim=1))\n",
    "print(Euclidean_distance(a, b))\n",
    "print(hyperbolic_distance(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPELoss(y_pred, y_true):\n",
    "        return MeanAbsolutePercentageError(y_pred, y_true)\n",
    "\n",
    "def MAPEonVAR(y_pred, y_true):\n",
    "        normed_var_pred = torch.var(y_pred, dim=0) / torch.mean(y_pred, dim=0)\n",
    "        normed_var_true = torch.var(y_true, dim=0) / torch.mean(y_true, dim=0)\n",
    "        return MeanAbsolutePercentageError(normed_var_pred, normed_var_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printlog(info):\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "    print(str(info)+\"\\n\")\n",
    "\n",
    "class StepRunner:\n",
    "    def __init__(self, net, loss_fn,\n",
    "                 stage = \"train\", metrics_dict = None, \n",
    "                 optimizer = None\n",
    "                 ):\n",
    "        self.model,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n",
    "        self.optimizer = optimizer\n",
    "            \n",
    "    def step(self, source_event, target_event, emd):\n",
    "        #loss\n",
    "        source_event, target_event, emd = source_event.to(device), target_event.to(device), emd.to(device)\n",
    "\n",
    "        source_emebedding = self.model(source_event)\n",
    "        target_embedding = self.model(target_event)\n",
    "        loss = self.loss_fn(source_emebedding, target_embedding, emd)\n",
    "        \n",
    "        #backward()\n",
    "        if self.optimizer is not None and self.stage==\"train\": \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "        #metrics\n",
    "        step_metrics = {self.stage+\"_\"+name:metric_fn(source_emebedding, target_embedding, emd).item() \n",
    "                        for name,metric_fn in self.metrics_dict.items()}\n",
    "        return loss.item(), step_metrics\n",
    "    \n",
    "    def train_step(self, source_event, target_event, emd):\n",
    "        self.model.train() \n",
    "        return self.step(source_event, target_event, emd)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def eval_step(self, source_event, target_event, emd):\n",
    "        self.model.eval() \n",
    "        return self.step(source_event, target_event, emd)\n",
    "    \n",
    "    def __call__(self, source_event, target_event, emd):\n",
    "        if self.stage==\"train\":\n",
    "            return self.train_step(source_event, target_event, emd) \n",
    "        else:\n",
    "            return self.eval_step(source_event, target_event, emd)\n",
    "        \n",
    "class EpochRunner:\n",
    "    def __init__(self,steprunner):\n",
    "        self.steprunner = steprunner\n",
    "        self.stage = steprunner.stage\n",
    "        \n",
    "    def __call__(self,dataloader):\n",
    "        total_loss,step = 0,0\n",
    "        loop = tqdm(enumerate(dataloader), total =len(dataloader), file = sys.stdout)\n",
    "        for batch_idx, (source_event, target_event, emd) in loop: \n",
    "            loss, step_metrics = self.steprunner(source_event, target_event, emd)\n",
    "            step_log = dict({self.stage+\"_loss\":loss},**step_metrics)\n",
    "            total_loss += loss\n",
    "            step+=1\n",
    "            if batch_idx!=len(dataloader)-1:\n",
    "                loop.set_postfix(**step_log)\n",
    "            else:\n",
    "                epoch_loss = total_loss/step\n",
    "                epoch_metrics = {self.stage+\"_\"+name:metric_fn.compute()\n",
    "                                 for name,metric_fn in self.steprunner.metrics_dict.items()}\n",
    "                epoch_log = dict({self.stage+\"_loss\":epoch_loss},**epoch_metrics)\n",
    "                loop.set_postfix(**epoch_log)\n",
    "\n",
    "                for name,metric_fn in self.steprunner.metrics_dict.items():\n",
    "                    metric_fn.reset()\n",
    "        return epoch_log\n",
    "\n",
    "@torch.compile()\n",
    "def train_model(net, optimizer, loss_fn, metrics_dict, \n",
    "                train_dataloader, val_dataloader=None, \n",
    "                epochs=10, ckpt_path='checkpoint.pt',\n",
    "                patience=5, monitor=\"MAPE\", mode=\"min\"):\n",
    "    \n",
    "    history = {}\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        printlog(\"Epoch {0} / {1}\".format(epoch, epochs))\n",
    "\n",
    "        # 1，train -------------------------------------------------  \n",
    "        train_step_runner = StepRunner(net = net,stage=\"train\",\n",
    "                loss_fn = loss_fn,metrics_dict=deepcopy(metrics_dict),\n",
    "                optimizer = optimizer)\n",
    "        train_epoch_runner = EpochRunner(train_step_runner)\n",
    "        train_metrics = train_epoch_runner(train_dataloader)\n",
    "\n",
    "        for name, metric in train_metrics.items():\n",
    "            history[name] = history.get(name, []) + [metric]\n",
    "\n",
    "        # 2，validate -------------------------------------------------\n",
    "        if val_dataloader:\n",
    "            val_step_runner = StepRunner(net = net,stage=\"val\",\n",
    "                loss_fn = loss_fn,metrics_dict=deepcopy(metrics_dict))\n",
    "            val_epoch_runner = EpochRunner(val_step_runner)\n",
    "            with torch.no_grad():\n",
    "                val_metrics = val_epoch_runner(val_dataloader)\n",
    "            val_metrics[\"epoch\"] = epoch\n",
    "            for name, metric in val_metrics.items():\n",
    "                history[name] = history.get(name, []) + [metric]\n",
    "\n",
    "        # 3，early-stopping -------------------------------------------------\n",
    "        arr_scores = history[monitor]\n",
    "        best_score_idx = np.argmax(arr_scores) if mode==\"max\" else np.argmin(arr_scores)\n",
    "        if best_score_idx==len(arr_scores)-1:\n",
    "            torch.save(net.state_dict(),ckpt_path)\n",
    "            print(\"<<<<<< reach best {0} : {1} >>>>>>\".format(monitor,\n",
    "                 arr_scores[best_score_idx]))\n",
    "        if len(arr_scores)-best_score_idx>patience:\n",
    "            print(\"<<<<<< {} without improvement in {} epoch, early stopping >>>>>>\".format(\n",
    "                monitor,patience))\n",
    "            break \n",
    "    net.load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "    return pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
