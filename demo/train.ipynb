{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime \n",
    "import numpy as np\n",
    "import math\n",
    "sys.path.append(\"..\")\n",
    "# from tqdm.rich import tqdm, trange\n",
    "from tqdm import tqdm, trange\n",
    "from copy import deepcopy\n",
    "import h5py\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchmetrics.regression import MeanAbsolutePercentageError\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from data_loader import get_database_path, get_h5_files, read_h5_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "random_seed = 114514\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Set default tensor type\n",
    "if sys.platform == \"darwin\":\n",
    "    # Mac OS\n",
    "    default_precision = torch.float32\n",
    "else:\n",
    "    # Linux or Windows\n",
    "    default_precision = torch.float64\n",
    "torch.set_default_dtype(default_precision)\n",
    "\n",
    "# Set device\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if sys.platform == \"darwin\" else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = get_database_path()\n",
    "bkg_files, sig_files = get_h5_files()\n",
    "\n",
    "\n",
    "# SM processes\n",
    "bkg = read_h5_file(database_path, bkg_files[0]['file'])\n",
    "bkg_pairs, bkg_emds = read_h5_file(database_path, \"bkg_emds.h5\", datatype='EMD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedEventsDataset(Dataset):\n",
    "    def __init__(self, events, pairs, emds):\n",
    "        assert len(emds) == len(pairs)\n",
    "        assert len(pairs.shape) == 2 and pairs.shape[1] == 2\n",
    "        self.events = torch.from_numpy(events[:, :, :3])\n",
    "        self.pairs = torch.from_numpy(pairs)\n",
    "        self.emds = torch.from_numpy(emds)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.events[self.pairs[idx][0]], self.events[self.pairs[idx][1]], self.emds[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13451915 67259575 67259575\n",
      "53807660\n"
     ]
    }
   ],
   "source": [
    "print(len(bkg), len(bkg_pairs), len(bkg_emds))\n",
    "train_val_split = 0.8\n",
    "train_size = int(len(bkg_pairs) * train_val_split)\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bkg_dataset = PairedEventsDataset(bkg, bkg_pairs[:train_size], bkg_emds[:train_size])\n",
    "val_bkg_dataset = PairedEventsDataset(bkg, bkg_pairs[train_size:], bkg_emds[train_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bkg_dataloader = DataLoader(train_bkg_dataset, batch_size=256, shuffle=False)\n",
    "val_bkg_dataloader = DataLoader(val_bkg_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        def make_layer(in_size, out_size):\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(in_size, out_size),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(0.1)\n",
    "            )\n",
    "        self.layers = nn.Sequential(\n",
    "            make_layer(input_size, hidden_sizes[0]),\n",
    "            *[make_layer(hidden_sizes[i], hidden_sizes[i+1]) for i in range(len(hidden_sizes)-1)],\n",
    "            nn.Linear(hidden_sizes[-1], output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('encoding', encoding.unsqueeze(0).transpose(0, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:x.size(0), :]\n",
    "\n",
    "    \n",
    "    \n",
    "class ParticleEventTransformer(nn.Module):\n",
    "    def __init__(self, feature_size, embed_size, num_heads, hidden_dim, output_dim, num_layers):\n",
    "        super(ParticleEventTransformer, self).__init__()\n",
    "        self.particle_embedding = nn.Linear(feature_size, embed_size)\n",
    "        self.pos_encoder = PositionalEncoding(embed_size)\n",
    "        encoder_layers = TransformerEncoderLayer(embed_size, num_heads, hidden_dim)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.output_layer = nn.Linear(embed_size * 19, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.particle_embedding(x)  # [batch_size, 19, embed_size]\n",
    "        x = x.permute(1, 0, 2)  # Transformer expects [seq_len, batch_size, embedding_dim]\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 0, 2)  # Switch back to [batch_size, seq_len, embedding_dim]\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class particleTransformer(nn.Module):\n",
    "    def __init__(self, particle_feature_size, d_model, nhead, num_encoder_layers, embed_dim, max_seq_length, pos_dropout, layer_widths):\n",
    "        super().__init__()  \n",
    "        self.d_model = d_model\n",
    "        self.embed_src = nn.Linear(particle_feature_size, d_model)\n",
    "        self.embed_tgt = nn.Linear(particle_feature_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)\n",
    "\n",
    "        #self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.NPART = max_seq_length\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat,out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(num_features=out_feat))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "            layers.append(nn.Dropout(p=0.5, inplace = False))\n",
    "            return layers\n",
    "\n",
    "\n",
    "        #layer_widths = [200,50,10]\n",
    "        self.fcblock = nn.Sequential(\n",
    "                                     *block(d_model*max_seq_length, layer_widths[0] ),\n",
    "                                     *[layers for i in range(len(layer_widths)-1) for layers in block(layer_widths[i],layer_widths[i+1])],\n",
    "                                     nn.Linear(layer_widths[-1], embed_dim)\n",
    "                                     )\n",
    "        #print(self.fcblock)\n",
    "\n",
    "\n",
    "    def forward(self, src):\n",
    "\n",
    "        src = src.permute(1,0,2)\n",
    "        #tgt = tgt.permute(1,0,2)\n",
    "\n",
    "        #src = self.embed_src(src)\n",
    "        src = self.pos_enc(self.embed_src(src) * math.sqrt(self.d_model))\n",
    "        #tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))\n",
    "        #output = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask,\n",
    "        #                          tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output.permute(1,0,2)\n",
    "        output = output.reshape(-1,self.d_model*self.NPART)\n",
    "        #print(self.fcblock)\n",
    "        output = self.fcblock(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/desmondhe/anaconda3/envs/ad/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "feature_size = 3  # pT, eta, phi\n",
    "embed_size = 16  # Dimension of embedding space\n",
    "num_heads = 4    # Number of attention heads\n",
    "hidden_dim = 256 # Dimension of the feedforward network model in nn.TransformerEncoder\n",
    "output_dim = 2  # Final dimension of the output embedding\n",
    "num_layers = 4   # Number of nn.TransformerEncoderLayer\n",
    "\n",
    "model = ParticleEventTransformer(feature_size, embed_size, num_heads, hidden_dim, output_dim, num_layers)\n",
    "# model = particleTransformer(feature_size, d_model=embed_size, nhead=num_heads, num_encoder_layers=num_layers, embed_dim=output_dim, max_seq_length=19, pos_dropout=0.1, layer_widths=[200,50,10])\n",
    "\n",
    "batch_size = 10\n",
    "events = torch.randn(batch_size, 19, 3)  # Random data simulating batch of events\n",
    "\n",
    "output_embeddings = model(events)\n",
    "print(output_embeddings.shape)  # Expected shape: [batch_size, output_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss & Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Euclidean_distance(y1, y2):\n",
    "    return torch.norm(y1 - y2, dim=1)\n",
    "\n",
    "def hyperbolic_distance(y1, y2):\n",
    "    return torch.acosh(1 + 2 * torch.sum((y1 - y2)**2, dim=1) / ((1 - torch.sum(y1**2, dim=1)) * (1 - torch.sum(y2**2, dim=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(10, 3)\n",
    "b = torch.randn(10, 3)\n",
    "print(Euclidean_distance(a, b).shape)\n",
    "print(hyperbolic_distance(a, b).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE = MeanAbsolutePercentageError().to(device)\n",
    "\n",
    "def MAPE_dispersion(original_distance, embed_distance):\n",
    "        dispersion_emd = torch.var(original_distance, dim=0) / torch.mean(original_distance, dim=0)\n",
    "        dispersion_ori = torch.var(embed_distance, dim=0) / torch.mean(embed_distance, dim=0)\n",
    "        return MAPE(dispersion_emd, dispersion_ori)\n",
    "\n",
    "def embed_ratio(embed_distance, original_distance):\n",
    "    return torch.mean(embed_distance / original_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricUpdater(nn.Module):\n",
    "    def __init__(self, metric_fn):\n",
    "        super(MetricUpdater, self).__init__()\n",
    "        self.metric_fn = metric_fn\n",
    "        self.total_metric = 0\n",
    "        self.count = 0\n",
    "    def forward(self, original, embed):\n",
    "        metric = self.metric_fn(embed, original)\n",
    "        self.update(metric)\n",
    "        return metric\n",
    "    def update(self, metric):\n",
    "        self.total_metric += metric\n",
    "        self.count += 1\n",
    "    def compute(self):\n",
    "        return self.total_metric / self.count\n",
    "    def reset(self):\n",
    "        self.total_metric = 0\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE_updater = MetricUpdater(MAPE)\n",
    "MAPE_dispersion_updater = MetricUpdater(MAPE_dispersion)\n",
    "embed_ratio_updater = MetricUpdater(embed_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict = {\"MAPE\": MAPE_updater, \"Var of MAPE\": MAPE_dispersion_updater, \"Embed Ratio\": embed_ratio_updater}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printlog(info):\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "    print(str(info)+\"\\n\")\n",
    "\n",
    "class StepRunner:\n",
    "    def __init__(self, net, loss_fn, dist_fn,\n",
    "                 stage = \"train\", metrics_dict = None, \n",
    "                 optimizer = None\n",
    "                 ):\n",
    "        self.model,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n",
    "        self.optimizer = optimizer\n",
    "        self.dist_fn = dist_fn\n",
    "            \n",
    "    def step(self, source_event, target_event, emd):\n",
    "        #loss\n",
    "        source_event, target_event, emd = source_event.to(device), target_event.to(device), emd.to(device)\n",
    "\n",
    "        source_emebedding = self.model(source_event)\n",
    "        target_embedding = self.model(target_event)\n",
    "        emb_distance = self.dist_fn(source_emebedding, target_embedding)\n",
    "        loss = self.loss_fn(emb_distance, emd)\n",
    "        \n",
    "        #backward()\n",
    "        if self.optimizer is not None and self.stage==\"train\": \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "        #metrics\n",
    "        step_metrics = {self.stage+\"_\"+name:(metric_fn(emb_distance, emd)).item()\n",
    "                        for name,metric_fn in self.metrics_dict.items()}\n",
    "        return loss.item(), step_metrics\n",
    "    \n",
    "    def train_step(self, source_event, target_event, emd):\n",
    "        self.model.train() \n",
    "        return self.step(source_event, target_event, emd)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def eval_step(self, source_event, target_event, emd):\n",
    "        self.model.eval() \n",
    "        return self.step(source_event, target_event, emd)\n",
    "    \n",
    "    def __call__(self, source_event, target_event, emd):\n",
    "        if self.stage==\"train\":\n",
    "            return self.train_step(source_event, target_event, emd) \n",
    "        else:\n",
    "            return self.eval_step(source_event, target_event, emd)\n",
    "        \n",
    "class EpochRunner:\n",
    "    def __init__(self,steprunner):\n",
    "        self.steprunner = steprunner\n",
    "        self.stage = steprunner.stage\n",
    "        \n",
    "    def __call__(self,dataloader):\n",
    "        total_loss,step = 0,0\n",
    "        loop = tqdm(enumerate(dataloader), total =len(dataloader), file = sys.stdout)\n",
    "        for batch_idx, (source_event, target_event, emd) in loop: \n",
    "            loss, step_metrics = self.steprunner(source_event, target_event, emd)\n",
    "            step_log = dict({self.stage+\"_loss\":loss},**step_metrics)\n",
    "            total_loss += loss\n",
    "            step+=1\n",
    "            if batch_idx!=len(dataloader)-1:\n",
    "                loop.set_postfix(**step_log)\n",
    "            else:\n",
    "                epoch_loss = total_loss/step\n",
    "                epoch_metrics = {self.stage+\"_\"+name:metric_fn.compute().item()\n",
    "                                 for name,metric_fn in self.steprunner.metrics_dict.items()}\n",
    "                epoch_log = dict({self.stage+\"_loss\":epoch_loss},**epoch_metrics)\n",
    "                loop.set_postfix(**epoch_log)\n",
    "\n",
    "                for name,metric_fn in self.steprunner.metrics_dict.items():\n",
    "                    metric_fn.reset()\n",
    "        return epoch_log\n",
    "\n",
    "@torch.compile()\n",
    "def train_model(net, optimizer, \n",
    "                loss_fn, dist_fn,\n",
    "                metrics_dict, \n",
    "                train_dataloader, val_dataloader=None, \n",
    "                epochs=10, ckpt_path='checkpoint.pt',\n",
    "                patience=5, monitor=\"train_MAPE\", mode=\"min\"):\n",
    "    \n",
    "    history = {}\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        printlog(\"Epoch {0} / {1}\".format(epoch, epochs))\n",
    "\n",
    "        # 1，train -------------------------------------------------  \n",
    "        train_step_runner = StepRunner(net = net,stage=\"train\", dist_fn=dist_fn,\n",
    "                loss_fn = loss_fn,metrics_dict=metrics_dict,\n",
    "                optimizer = optimizer)\n",
    "        train_epoch_runner = EpochRunner(train_step_runner)\n",
    "        train_metrics = train_epoch_runner(train_dataloader)\n",
    "\n",
    "        for name, metric in train_metrics.items():\n",
    "            history[name] = history.get(name, []) + [metric]\n",
    "\n",
    "        # 2，validate -------------------------------------------------\n",
    "        if val_dataloader:\n",
    "            val_step_runner = StepRunner(net = net,stage=\"val\",\n",
    "                loss_fn = loss_fn, dist_fn=dist_fn,metrics_dict=metrics_dict)\n",
    "            val_epoch_runner = EpochRunner(val_step_runner)\n",
    "            with torch.no_grad():\n",
    "                val_metrics = val_epoch_runner(val_dataloader)\n",
    "            val_metrics[\"epoch\"] = epoch\n",
    "            for name, metric in val_metrics.items():\n",
    "                history[name] = history.get(name, []) + [metric]\n",
    "\n",
    "        # 3，early-stopping -------------------------------------------------\n",
    "            arr_scores = history[monitor]\n",
    "        best_score_idx = np.argmax(arr_scores) if mode==\"max\" else np.argmin(arr_scores)\n",
    "        if best_score_idx==len(arr_scores)-1:\n",
    "            torch.save(net.state_dict(),ckpt_path)\n",
    "            print(\"<<<<<< reach best {0} : {1} >>>>>>\".format(monitor,\n",
    "                 arr_scores[best_score_idx]))\n",
    "        if len(arr_scores)-best_score_idx>patience:\n",
    "            print(\"<<<<<< {} without improvement in {} epoch, early stopping >>>>>>\".format(\n",
    "                monitor,patience))\n",
    "            break \n",
    "    net.load_state_dict(torch.load(ckpt_path))\n",
    "    return pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "loss_fn = MAPE\n",
    "# dist_fn = Euclidean_distance\n",
    "dist_fn = hyperbolic_distance\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================2024-04-28 21:16:40\n",
      "Epoch 1 / 5\n",
      "\n",
      " 33%|███▎      | 69773/210187 [31:44<1:02:14, 37.60it/s, train_Embed Ratio=2.16, train_MAPE=1.29, train_Var of MAPE=0.203, train_loss=0.375]   "
     ]
    }
   ],
   "source": [
    "dfhistory = train_model(model, optimizer, loss_fn, dist_fn, metric_dict, train_dataloader=train_bkg_dataloader, val_dataloader=val_bkg_dataloader,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
